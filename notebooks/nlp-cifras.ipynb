{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "326a24fb",
   "metadata": {},
   "source": [
    "# Cifra Genre Classification\n",
    "\n",
    "- In this notebook, we will use FastAI and HuggingFace libraries to fine-tune deep learning models for music genre classification on our dataset.\n",
    "- These libraries help us leverage state-of-the-art models like RNNs and Transformers.\n",
    "- Initially, we'll explore using artist genre for prediction, but keep in mind this might not be very accurate due to genre variations within an artist's work.\n",
    "- Since music genre classification is often linked to song content, we can adapt successful NLP (Natural Language Processing) techniques for text classification to analyze song cifras and predict genre.\n",
    "- These models can be used to predict genres for unlabeled data (\"nao-informada\") in our dataset.\n",
    "- While RNNs are capable of text generation, generating cifras is a more specialized task that might require additional techniques beyond basic RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a922bff",
   "metadata": {},
   "source": [
    "## Dataset available\n",
    "\n",
    "Let's refresh the dataset we have consolidated and check some possible issues to deep learning model fine-tuning: \n",
    "1. There are too many artist genres;\n",
    "2. The distribution of cifras is not balanced accross different genres.\n",
    "3. Some genres have only a few samples.\n",
    "\n",
    "Therefore, let's select only the top 7 well-known Brazilian music genres, excluding some redundant classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe7ac3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cifra_url</th>\n",
       "      <th>url_artist_name</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>artist_genre</th>\n",
       "      <th>url_song_name</th>\n",
       "      <th>song_name</th>\n",
       "      <th>cifra_file_loc</th>\n",
       "      <th>cifra_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>60561</td>\n",
       "      <td>60561</td>\n",
       "      <td>60561</td>\n",
       "      <td>60561</td>\n",
       "      <td>60561</td>\n",
       "      <td>60561</td>\n",
       "      <td>60561</td>\n",
       "      <td>60561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>60561</td>\n",
       "      <td>15271</td>\n",
       "      <td>15953</td>\n",
       "      <td>73</td>\n",
       "      <td>48463</td>\n",
       "      <td>49596</td>\n",
       "      <td>60561</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>https://www.cifras.com.br/cifra/gesil-amarante...</td>\n",
       "      <td>hinos-avulsos-ccb</td>\n",
       "      <td>Hinos Avulsos Ccb</td>\n",
       "      <td>nao-informada</td>\n",
       "      <td>saudade</td>\n",
       "      <td>Saudade</td>\n",
       "      <td>cifras/gesil-amarante-jr/sou-um-milagre.txt</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>306</td>\n",
       "      <td>293</td>\n",
       "      <td>14746</td>\n",
       "      <td>62</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>9637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                cifra_url    url_artist_name  \\\n",
       "count                                               60561              60561   \n",
       "unique                                              60561              15271   \n",
       "top     https://www.cifras.com.br/cifra/gesil-amarante...  hinos-avulsos-ccb   \n",
       "freq                                                    1                306   \n",
       "\n",
       "              artist_name   artist_genre url_song_name song_name  \\\n",
       "count               60561          60561         60561     60561   \n",
       "unique              15953             73         48463     49596   \n",
       "top     Hinos Avulsos Ccb  nao-informada       saudade   Saudade   \n",
       "freq                  293          14746            62        61   \n",
       "\n",
       "                                     cifra_file_loc cifra_key  \n",
       "count                                         60561     60561  \n",
       "unique                                        60561        37  \n",
       "top     cifras/gesil-amarante-jr/sou-um-milagre.txt         G  \n",
       "freq                                              1      9637  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cifras_df = pd.read_csv(\"valid_cifras.csv\", index_col=0)\n",
    "cifras_df.describe(include=[object])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbb03331",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['gospel', 'mpb', 'indie-rock', 'rockn-roll', 'nao-informada',\n",
       "       'catolicas', 'sertanejo', 'infantis', 'brasil', 'worship',\n",
       "       'jovem-guarda', 'pisadinha', 'rock-alternativo', 'forro',\n",
       "       'punk-rock', 'diversos', 'gauchas', 'romantica', 'pop-music',\n",
       "       'funk-carioca', 'folk', 'samba-e-pagode', 'pop-rock',\n",
       "       'velha-guarda', 'oldies', 'latinas', 'regional', 'musica-crista',\n",
       "       'reggae', 'eletronica', 'raphip-hop', 'axe-music', 'brega',\n",
       "       'espiritas', 'rock-classico', 'indie', 'heavy-metal', 'country',\n",
       "       'indie-pop', 'funk', 'bossa-nova', 'rb', 'trap', 'fado', 'musical',\n",
       "       'blues', 'indian', 'Samba-Rock', 'besteirol', 'autoconhecimento',\n",
       "       'rock-grunge', 'musica-nativista', 'hinos', 'samba-enredo',\n",
       "       'australiano', 'thrash-metal', 'umbanda', 'jazz', 'novelas',\n",
       "       'jingle', 'rock-gotico', 'choro', 'soul', 'filmes', 'lambada',\n",
       "       'reggaeton', 'disco', 'k-pop', 'especial-de-natal', 'numetal',\n",
       "       'dance-music', 'opera', 'mangue'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifras_df.artist_genre.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "796eb65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cifras_df.artist_genre.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "453e1497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nao-informada', 14746],\n",
       " ['gospel', 9902],\n",
       " ['sertanejo', 8058],\n",
       " ['mpb', 4291],\n",
       " ['samba-e-pagode', 2527],\n",
       " ['forro', 2509],\n",
       " ['pop-music', 2198],\n",
       " ['catolicas', 1907],\n",
       " ['rockn-roll', 1872],\n",
       " ['diversos', 1466]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres = [[genre,len(cifras_df[cifras_df['artist_genre']==genre])] for genre in cifras_df.artist_genre.unique()]\n",
    "sorted(genres,key=lambda x: x[1], reverse=True)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f431cb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected genres: ['gospel', 'sertanejo', 'mpb', 'samba-e-pagode', 'forro', 'pop-music', 'rockn-roll']\n"
     ]
    }
   ],
   "source": [
    "selected_genres = ['gospel','sertanejo','mpb','samba-e-pagode','forro','pop-music','rockn-roll']\n",
    "print(\"Selected genres: \" + str(selected_genres))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de49e27",
   "metadata": {},
   "source": [
    "#### FastAI expects labels of the data to be organized in a certain way:\n",
    "- \"train\" and \"test\" folders will split the dataset into training and validation data.\n",
    "- In each folder, there will be folders with each classification possible, so we will organize the cifras folder with genres instead of artists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7a5127",
   "metadata": {},
   "source": [
    "## Dataset Path Organization and balance data\n",
    "\n",
    "- Reorganize the paths according to what FastAI expects.\n",
    "- Balance the data getting randomly 1500 cifras for each genre.\n",
    "- Split dataset into training 80% and validation 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93540230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "num_cifras = 10\n",
    "train_prop = 0.8\n",
    "\n",
    "new_folder ='selected_cifras_test'\n",
    "\n",
    "def organize_df_to_training(new_folder, num_cifras, train_prop, random_seed=42):\n",
    "\n",
    "    train_folder = new_folder + '/train'\n",
    "    valid_folder = new_folder + \"/test\"\n",
    "\n",
    "    # Check if new folder already exists\n",
    "    if os.path.isdir(new_folder):\n",
    "        # Remove folder if already exists\n",
    "        shutil.rmtree(new_folder)\n",
    "\n",
    "    # Create new folder\n",
    "    os.makedirs(new_folder)\n",
    "    os.makedirs(train_folder)\n",
    "    os.makedirs(valid_folder)\n",
    "\n",
    "    # Loop through all selected genres\n",
    "    for genre in selected_genres:\n",
    "\n",
    "        # Create subfolders for that genre\n",
    "        new_train_folder = train_folder + \"/\" + genre\n",
    "        os.makedirs(new_train_folder)\n",
    "        new_train_path = Path(new_train_folder)\n",
    "\n",
    "        new_valid_folder = valid_folder + \"/\" + genre\n",
    "        os.makedirs(new_valid_folder)\n",
    "        new_valid_path = Path(new_valid_folder)\n",
    "\n",
    "        # Get all itens for that selected genre\n",
    "        subset_df = cifras_df[cifras_df[\"artist_genre\"] == genre]\n",
    "        # Take some samples of cifras for that genre\n",
    "        samples = subset_df.sample(num_cifras, random_state=random_seed)\n",
    "\n",
    "        #Split into train and valid\n",
    "        train_samples = samples.iloc[:int(len(samples)*train_prop)]\n",
    "        valid_samples = samples.iloc[int(len(samples)*train_prop):]\n",
    "\n",
    "        # Copy training samples\n",
    "        for index, row in train_samples.iterrows():    \n",
    "            if Path(row.cifra_file_loc).is_file():\n",
    "                # Copy file to new folder while adding artist name to cifra file name\n",
    "                new_location = new_train_path / (row.url_artist_name + \"_\" + row.url_song_name + \".txt\" )\n",
    "\n",
    "                shutil.copy(row.cifra_file_loc, new_location)\n",
    "            else:\n",
    "                print(\"File not found!\")\n",
    "\n",
    "        # Copy validation samples\n",
    "        for index, row in valid_samples.iterrows():    \n",
    "            if Path(row.cifra_file_loc).is_file():\n",
    "                # Copy file to new folder while adding artist name to cifra file name\n",
    "                new_location = new_valid_path / (row.url_artist_name + \"_\" + row.url_song_name + \".txt\" )\n",
    "\n",
    "                shutil.copy(row.cifra_file_loc, new_location)\n",
    "            else:\n",
    "                print(f\"Cifra File {row.cifra_file_loc} not found!\")\n",
    "\n",
    "\n",
    "num_cifras = 10 # Number of cifras for each genre\n",
    "train_prop = 0.8 # Proportion of cifras that will be to training, the rest will be for validation\n",
    "\n",
    "new_folder ='selected_cifras_test' # Name of folder where training dataset will be stored\n",
    "\n",
    "organize_df_to_training(new_folder, num_cifras, train_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8675ff4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mselected_cifras_test\u001b[0m\r\n",
      "├── \u001b[01;34mtest\u001b[0m\r\n",
      "│   ├── \u001b[01;34mforro\u001b[0m\r\n",
      "│   │   ├── avioes-do-forro_seu-choro-nao-me-faz-desistir.txt\r\n",
      "│   │   └── wesley-safadao_amiga-parceira.txt\r\n",
      "│   ├── \u001b[01;34mgospel\u001b[0m\r\n",
      "│   │   ├── comunidade-doce-mae-de-deus_apenas-comecou.txt\r\n",
      "│   │   └── julio-cesar-e-marlene_foi-ele.txt\r\n",
      "│   ├── \u001b[01;34mmpb\u001b[0m\r\n",
      "│   │   ├── nilo-amaro-e-seus-cantores-de-ebano_urutau.txt\r\n",
      "│   │   └── tie_a-bailarina-e-o-astronauta.txt\r\n",
      "│   ├── \u001b[01;34mpop-music\u001b[0m\r\n",
      "│   │   ├── junior-de-oliveira_jeitinho-perfeito.txt\r\n",
      "│   │   └── wanessa-camargo_vou-lembrar.txt\r\n",
      "│   ├── \u001b[01;34mrockn-roll\u001b[0m\r\n",
      "│   │   ├── cor-do-invisivel_sosia-em-ideias.txt\r\n",
      "│   │   └── divididos_ojos-de-rio.txt\r\n",
      "│   ├── \u001b[01;34msamba-e-pagode\u001b[0m\r\n",
      "│   │   ├── grupo-na-hora-h_no-batuque-do-meu-samba.txt\r\n",
      "│   │   └── os-originais-do-samba_tenha-fe-pois-amanha-um-lindo-dia-vai-nascer.txt\r\n",
      "│   └── \u001b[01;34msertanejo\u001b[0m\r\n",
      "│       ├── roberta-miranda_gracas-a-deus.txt\r\n",
      "│       └── tonico-e-tinoco_historia-da-minha-vida.txt\r\n",
      "└── \u001b[01;34mtrain\u001b[0m\r\n",
      "    ├── \u001b[01;34mforro\u001b[0m\r\n",
      "    │   ├── arriba-saia_junho.txt\r\n",
      "    │   ├── dominguinhos_querubim.txt\r\n",
      "    │   ├── garota-safada_fora-do-eixo.txt\r\n",
      "    │   ├── garota-safada_o-pegador.txt\r\n",
      "    │   ├── juliette_diferenca-mara.txt\r\n",
      "    │   ├── libio_isso-e-imortal.txt\r\n",
      "    │   ├── miltinho_se-voce-visse.txt\r\n",
      "    │   └── pedra-benta_toma-jeito.txt\r\n",
      "    ├── \u001b[01;34mgospel\u001b[0m\r\n",
      "    │   ├── edenia-morais_obrigado-jesus.txt\r\n",
      "    │   ├── irmas-leonel_o-servo-tem-consolador.txt\r\n",
      "    │   ├── jessica-augusto_sou-mais.txt\r\n",
      "    │   ├── jose-wantuil_muralhas-gigantes.txt\r\n",
      "    │   ├── marcus-salles_quero-ser-teu.txt\r\n",
      "    │   ├── ministerio-sao-miguel-arcanjo_aruja.txt\r\n",
      "    │   ├── padre-rodrigo-natal_liberta-me-senhor-jesus.txt\r\n",
      "    │   └── testemunhas-de-jeova_quero-ser-como-noe.txt\r\n",
      "    ├── \u001b[01;34mmpb\u001b[0m\r\n",
      "    │   ├── caetano-veloso_jeito-de-corpo.txt\r\n",
      "    │   ├── lo-borges_nenhum-misterio.txt\r\n",
      "    │   ├── lucinha-lins_espelhos-de-camarim.txt\r\n",
      "    │   ├── ney-matogrosso_belissima.txt\r\n",
      "    │   ├── oswaldo-montenegro_por-descuido-ou-displicencia.txt\r\n",
      "    │   ├── pery-ribeiro_na-baixa-do-sapateiro.txt\r\n",
      "    │   ├── valeria-costa_nuvens-no-ceu.txt\r\n",
      "    │   └── vitor-hugo_hobin-hood-da-paixao.txt\r\n",
      "    ├── \u001b[01;34mpop-music\u001b[0m\r\n",
      "    │   ├── alejandro-alonso_no-me-digas.txt\r\n",
      "    │   ├── d-zero_me-estoy-enamorando.txt\r\n",
      "    │   ├── iza_esse-brilho-e-meu.txt\r\n",
      "    │   ├── jorge-palma_jeremias-o-fora-da-lei.txt\r\n",
      "    │   ├── mallu-magalhaes_america-latina.txt\r\n",
      "    │   ├── moderatto_desatados.txt\r\n",
      "    │   ├── rauw-alejandro_curame.txt\r\n",
      "    │   └── sandy-junior_e-cedo-pra-amar-assim.txt\r\n",
      "    ├── \u001b[01;34mrockn-roll\u001b[0m\r\n",
      "    │   ├── alejandro-lerner_no-te-aguanto-mas.txt\r\n",
      "    │   ├── charly-garcia_demasiado-ego-one-to-one.txt\r\n",
      "    │   ├── despistaos_mi-accidente-preferido.txt\r\n",
      "    │   ├── engenheiros-do-hawaii_eternas-ondas.txt\r\n",
      "    │   ├── jaguares_como-tu.txt\r\n",
      "    │   ├── leusemia_miradas-incompletas.txt\r\n",
      "    │   ├── selvagens-a-procura-de-lei_mucambo-cafundo.txt\r\n",
      "    │   └── video-hits_louco-por-voce.txt\r\n",
      "    ├── \u001b[01;34msamba-e-pagode\u001b[0m\r\n",
      "    │   ├── dilsinho_terra-do-nunca.txt\r\n",
      "    │   ├── grupo-deixa-rolar_so-por-acaso.txt\r\n",
      "    │   ├── katinguele_axe-a-sua-fe.txt\r\n",
      "    │   ├── oba-oba-samba-house_dessa-vez-aprendo.txt\r\n",
      "    │   ├── pegadapan_ja-descobri.txt\r\n",
      "    │   ├── raca-negra_assim.txt\r\n",
      "    │   ├── raca-negra_cheias-de-manias.txt\r\n",
      "    │   └── toniquinho-batuqueiro_ditado-antigo.txt\r\n",
      "    └── \u001b[01;34msertanejo\u001b[0m\r\n",
      "        ├── conrado-e-aleksandro_chorando-mamado.txt\r\n",
      "        ├── gusttavo-lima_minha-estrela-perdida.txt\r\n",
      "        ├── jads-e-jadson_toca-um-joao-mineiro-e-marciano.txt\r\n",
      "        ├── leonardo_homem-e-que-nem-lata.txt\r\n",
      "        ├── lucas-silva_agenda-rabiscada.txt\r\n",
      "        ├── luiza-e-maurilio_s-de-saudade-part-ze-neto-e-cristiano.txt\r\n",
      "        ├── marcos-e-belutti_um-dia-a-gente-se-encontra.txt\r\n",
      "        └── ze-neto-e-cristiano_bebi-minha-bicicleta.txt\r\n",
      "\r\n",
      "17 directories, 70 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree selected_cifras_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2825d9",
   "metadata": {},
   "source": [
    "- The previous code is capable of copying a number of samples from our dataset into an organized folder according to FastAI standards.\n",
    "- We run the code for a couple of samples only, and checked that it is splitting the dataset correctly.\n",
    "- Now we can generate the split of 1500 cifras for each music genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6248cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cifras = 1500 # Number of cifras for each genre\n",
    "train_prop = 0.8 # Proportion of cifras that will be to training, the rest will be for validation\n",
    "\n",
    "new_folder ='selected_cifras' # Name of folder where training dataset will be stored\n",
    "\n",
    "organize_df_to_training(new_folder, num_cifras, train_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "850efaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mselected_cifras\u001b[0m\r\n",
      "├── \u001b[01;34mtest\u001b[0m\r\n",
      "│   ├── \u001b[01;34mforro\u001b[0m  [300 entries exceeds filelimit, not opening dir]\r\n",
      "│   ├── \u001b[01;34mgospel\u001b[0m  [300 entries exceeds filelimit, not opening dir]\r\n",
      "│   ├── \u001b[01;34mmpb\u001b[0m  [300 entries exceeds filelimit, not opening dir]\r\n",
      "│   ├── \u001b[01;34mpop-music\u001b[0m  [300 entries exceeds filelimit, not opening dir]\r\n",
      "│   ├── \u001b[01;34mrockn-roll\u001b[0m  [300 entries exceeds filelimit, not opening dir]\r\n",
      "│   ├── \u001b[01;34msamba-e-pagode\u001b[0m  [300 entries exceeds filelimit, not opening dir]\r\n",
      "│   └── \u001b[01;34msertanejo\u001b[0m  [300 entries exceeds filelimit, not opening dir]\r\n",
      "└── \u001b[01;34mtrain\u001b[0m\r\n",
      "    ├── \u001b[01;34mforro\u001b[0m  [1200 entries exceeds filelimit, not opening dir]\r\n",
      "    ├── \u001b[01;34mgospel\u001b[0m  [1200 entries exceeds filelimit, not opening dir]\r\n",
      "    ├── \u001b[01;34mmpb\u001b[0m  [1200 entries exceeds filelimit, not opening dir]\r\n",
      "    ├── \u001b[01;34mpop-music\u001b[0m  [1200 entries exceeds filelimit, not opening dir]\r\n",
      "    ├── \u001b[01;34mrockn-roll\u001b[0m  [1200 entries exceeds filelimit, not opening dir]\r\n",
      "    ├── \u001b[01;34msamba-e-pagode\u001b[0m  [1200 entries exceeds filelimit, not opening dir]\r\n",
      "    └── \u001b[01;34msertanejo\u001b[0m  [1200 entries exceeds filelimit, not opening dir]\r\n",
      "\r\n",
      "17 directories, 0 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree selected_cifras --filelimit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd9d351",
   "metadata": {},
   "source": [
    "## FastAI Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f6a2cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b25699",
   "metadata": {},
   "source": [
    "- After organizing the folder and balacing the data for the top 7 music genres in the dataset, we can use FastAI DataLoaders and check what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28dc41ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos [ intro]dução : f xxmaj dm xxmaj bb \\n\\n xxup verso 1 \\n\\n f xxmaj dm \\n xxmaj pisando fundo , acelerando tudo \\n xxmaj bb f xxup f7 f \\n xxmaj xxunk saindo do limite \\n f xxup f7 f xxmaj dm xxmaj bb xxmaj bb9 f xxup f7 f \\n é o que eu te disse eu sou assim , partindo pra cima , fugindo de mim \\n f xxup f7 f xxmaj dm \\n xxmaj eu corro muito , eu vou pra todo lado \\n xxmaj bb f xxup f7 f \\n xxmaj levando comigo quem ta do meu lado xxunk … \\n f xxmaj dm xxmaj bb xxmaj bb9 f xxup f7 f \\n é o que eu te disse eu sou assim , partindo pra cima , fugindo de mim \\n\\n pré - refrão \\n\\n xxmaj bb xxmaj dm \\n xxmaj ah não perco</td>\n",
       "      <td>rockn-roll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos xxmaj abrir tablatura \\n▁\\n▁\\n▁ xxup intro ( freely ) \\n▁ xxmaj em xxmaj bm \\n▁ 1 + 2 + 3 + 4 + 1 + 2 + 3 + 4 + 1 + 2 + 3 + 4 + \\n e| xxrep 16 - | xxrep 16 - | xxrep 3 - +7 xxrep 11 - | \\n xxup b| xxrep 14 - 0-| xxrep 16 - | xxrep 3 - +7 xxrep 11 - | \\n xxup g| xxrep 12 - 0 xxrep 3 - | xxrep 16 - | xxrep 3 - +7 xxrep 11 - | \\n xxup d| xxrep 4 - 0 xxrep 3 - 2 / 4 xxrep 5 - \\ 2 - 0 xxrep 9 - 0 xxrep 3 - | xxrep 16 - | \\n xxmaj a|0h2 xxrep 13 - | xxrep 4 - 2 - 0 - h2 xxrep 6 -</td>\n",
       "      <td>pop-music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos [ intro ] ( base del tema ) \\n\\n▁ d f d f d f d \\n xxmaj viaje de xxunk de sombras y cielos \\n▁ d f d f d xxmaj dsus4 \\n xxmaj mezcla de muerte y amor \\n▁ d f d f d f d \\n xxmaj pies que xxunk xxunk el suelo \\n▁ d f d f d xxmaj riff1 x2 \\n xxmaj mezcla de miedo y amor \\n\\n▁ xxmaj riff1 \\n xxmaj no son tus ojos ni tu bandera \\n▁ xxmaj riff1 \\n xxmaj no son tus ojos ni tu bandera \\n\\n d f d f d f d \\n xxmaj eso que dicen a veces no es cierto \\n d f d f d \\n xxmaj esto no es un desierto \\n▁ d f d f d f d \\n xxmaj nubes que siempre se caen del cielo \\n▁ d f d f</td>\n",
       "      <td>rockn-roll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxbos [ intro ] d7m(9 ) xxup d6(9 ) xxup d7m(9 ) xxup d6(9 ) xxup d7m(9 ) \\n▁ bb7m(9 ) bb7 m bb7m(9 ) bb7 m \\n▁ bb7m(9 ) bb7 m bb7m(9 ) \\n\\n xxup d7m(9 ) \\n xxmaj melhor eu ir \\n▁ xxup g7 m f # m7 xxup g7 m \\n xxmaj tudo bem vai ser melhor só \\n▁ xxup d7m(9 ) \\n xxmaj se teve que ser assim \\n▁ xxup g7 m xxup c7(4 ) xxup b7(4 ) xxmaj bb7(4 ) xxup a7(4 ) \\n é que pensando bem nunca existiu nós \\n▁ xxup d7m(9 ) \\n xxup só eu que pensei na gente \\n▁ xxup g7 m f # m7 xxup g7 m \\n xxmaj ainda que demorei pra terminar , dói \\n▁ xxmaj bm7 xxup a9 \\n xxmaj não era só comigo que você ficava \\n▁ xxup g7 m d / f # f</td>\n",
       "      <td>samba-e-pagode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xxbos xxmaj intro : f xxmaj gm c xxmaj dm \\n\\n [ verso 1 ] \\n▁ xxmaj gm \\n xxmaj eu sei que eu me atrasei \\n▁ c f \\n xxmaj desculpa eu vacilei , pode dizer \\n▁ xxmaj dm \\n xxmaj eu só vou ouvir porque \\n\\n▁ xxmaj gm c \\n xxmaj ultimamente , você anda carente \\n▁ f \\n xxmaj diz que só foi mais uma \\n▁ xxmaj dm \\n xxmaj das mancadas que eu dei \\n\\n [ verso 2 ] \\n▁ xxmaj gm c \\n xxmaj pensando bem , são xxunk de um fim \\n▁ f xxmaj dm \\n xxmaj parece o limite , qualquer coisa é pretexto aqui \\n▁ xxmaj gm c \\n xxmaj uns minutos de espera , você vira uma fera \\n▁ f xxup f7 \\n e diz que não confia em mim \\n\\n [ pré - refrão ] \\n▁ xxmaj gm \\n xxmaj</td>\n",
       "      <td>sertanejo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xxbos xxmaj parte 1 c # m b a e \\n▁ c # m b e a \\n\\n ( guitarra 1 ) \\n\\n▁ c # m b a e \\n xxup e| xxrep 30 - | \\n xxup b|-5 - 5 - 5 - 5 - 4 - 4 - 4 - 4 - 2 - 2 - 2 - 2 - 9 - -9-| \\n xxup g| xxrep 30 - | \\n xxup d|-6 - 6 - 6 - 6 - 4 - 4 - 4 - 4 - 2 - 2 - 2 - 2 - 9 - -9-| \\n xxup a| xxrep 30 - | \\n xxup e| xxrep 30 - | \\n\\n▁ c # m b e a \\n xxup e| xxrep 30 - | \\n xxup b|-5 - 5 - 5 - 5 - 4 - 4 - 4 - 4 - 9 - 9</td>\n",
       "      <td>gospel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xxbos xxmaj abrir tablatura \\n▁\\n▁\\n▁\\n xxup intro \\n▁ e xxrep 49 - \\n▁ b xxrep 49 - \\n▁ g xxrep 49 - \\n▁ d xxrep 12 - 2 xxrep 5 - 2 xxrep 3 - 2 xxrep 12 - 2 xxrep 5 - 2 xxrep 3 - 2 xxrep 3 - \\n▁ a xxrep 12 - 3 xxrep 5 - 3 xxrep 3 - 3 xxrep 12 - 5 xxrep 5 - 5 xxrep 3 - 5 xxrep 3 - \\n▁ e -0 - 3 - 5 - -5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - -0 - 5 - 3 - 3 - 3 - 3 - 3 - 3 - 3 - 3 - 3 xxrep 3 - \\n\\n xxup riff 1 \\n▁ e xxrep 49 - \\n▁ b xxrep 49 - \\n▁ g xxrep 33 - 7 - 7</td>\n",
       "      <td>pop-music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xxbos xxmaj intro d / a a \\n▁ c / a g / a c / a g / a \\n▁ d / a a f / a g / a \\n▁ d / a a \\n▁ c / a g / a c / a g / a \\n▁ f g a g a \\n\\n [ verso 1 ] \\n\\n a a / c # \\n xxmaj este é um tempo de festa \\n▁ d a / c # \\n xxmaj este é um tempo de louvor \\n▁ xxmaj bm xxup e4 a d / f # \\n xxmaj pra celebrar aquele que primeiro nos amou \\n e / g # a g / a \\n xxmaj transformou nosso choro em riso \\n▁ d a / c # f # m \\n xxmaj nos deu novas vestes de louvor \\n▁ g xxup e4 a \\n xxmaj pra celebrar aquele</td>\n",
       "      <td>gospel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xxbos xxmaj intro f # c # d # m a # m \\n▁ b f # g # m7 c # \\n▁ f # c # d # m a # m \\n▁ b f # g # m c # \\n\\n f # \\n xxup lá vai mais um sonhador \\n▁ c # 4 c # \\n xxmaj falavam assim de mim \\n g # m f # b \\n xxmaj ele é só mais um caso comum \\n▁ c # 4 c # \\n xxmaj xxunk meu fim \\n f # \\n xxup lá vai mais um sonhador \\n▁ c # 4 c # \\n xxmaj falavam também de xxmaj josé \\n g # m \\n xxmaj mas xxmaj josé não xxunk \\n f # b \\n xxmaj não reclamou \\n▁ c # 4 c # \\n xxmaj segurou a fé \\n\\n▁ d # m \\n xxmaj</td>\n",
       "      <td>gospel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_path = Path(new_folder)\n",
    "dls = TextDataLoaders.from_folder(ds_path, valid='test')\n",
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080a7d39",
   "metadata": {},
   "source": [
    "- Learner from FastAI will define the pipeline of training the RNN.\n",
    "- By default for text classification, FastAI uses Long Short-Term Memory Networks (LSTM).\n",
    "- The metric we will use if the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f03b8792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='105070592' class='' max='105067061' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [105070592/105067061 04:12&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf557a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='25' class='' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      75.76% [25/33 04:10&lt;01:20 1.7695]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n",
      "/home/ymarca/anaconda3/envs/deeplearning/lib/python3.8/site-packages/fastai/torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  return getattr(torch, 'has_mps', False)\n"
     ]
    }
   ],
   "source": [
    "learn.fine_tune(4, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b58f24",
   "metadata": {},
   "source": [
    "- Sadly, my no-GPU notebook is not able to handle the fine-tuning process.\n",
    "- Also, there are many warnings from PyTorch yield.\n",
    "- Let's save the dataset to run in another notebook with more processing capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49c6801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -czf selected_cifras.tar.gz selected_cifras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934088e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HuggingFace Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bfe5604",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e20476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad774363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '[Intro ]D \\n\\n    Bm       A     D            Em    Bm      A4 \\nSe Cristo me libertar verdadeiramente livre serei \\n    Bm       A     D            Em    A      G \\nSe Cristo me libertar verdadeiramente livre serei \\n   D  F#m7           Bm       G      A \\nLiberta-me, senhor Jesus, senhor Jesus \\n  D   F#m7          Bm     A        D2 \\nLiberta-me, senhor Jesus, senhor Jesus \\n\\n    Bm       A     D            Em    Bm      A4 \\nSe Cristo me libertar verdadeiramente livre serei \\n    Bm       A     D            Em    A      G \\nSe Cristo me libertar verdadeiramente livre serei \\n   D  F#m7           Bm      G      A \\nLiberta-me, senhor Jesus, senhor Jesus \\n  D   F#m7          Bm7     A       G \\nLiberta-me, senhor Jesus, senhor Jesus \\n\\n( F#m  Bm7  A/C# ) \\n\\n G A  Bm   A \\nJesus, Jesus \\n G A  Bm   A \\nJesus, Jesus \\n G A  Bm   A \\nJesus, Jesus \\n G A  Bm   A \\nJesus, Jesus \\n\\n   D  F#m7           Bm      G      A \\nLiberta-me, senhor Jesus, senhor Jesus \\n  D   F#m7          Bm7     A       G \\nLiberta-me, senhor Jesus, senhor Jesus \\n\\n( D  F#m7  Bm7  G  A ) \\n\\n D   F#m7          Bm7     A        G \\nSalva-me, senhor Jesus, senhor Jesus \\n\\nFinal G  A  Bm7  A  G2', 'label': 'gospel'}\n",
      "{'text': 'C9       C7+         F9 \\nNão há nenhum outro como Deus \\n     Am7       G           F9  Am7  G  F9  Am7  G F9 \\nNão há nenhum outro como Deus \\n\\nC9                      G                     Am7          F9 \\n  Quanto mais eu me entrego mais eu quero ser Teu, Teu, Teu \\nC9                   G                      Am7          F9 \\n  Quanto mais eu te adoro mais eu quero ser Teu, Teu, Teu \\n\\nAm7        F9                C9               G/B \\n  Te conhecia de ouvir falar,  mas agora de contigo andar \\nAm7       G                       F9 \\n  Eu sei,  que não há Deus como o nosso Deus \\n\\n    C9        G          F9      Am7       G          F9 \\nNão há nenhum outro como Deus Não há nenhum outro como Deus \\n    Am7       G          F9      Am7       G          F9 \\nTu és maior, és o autor da fé Não há nenhum outro como Deus \\n\\nAm7  G  F9  Am7  G F9 \\n\\nC9                      G                     Am7          F9 \\n  Quanto mais eu me entrego mais eu quero ser Teu, Teu, Teu \\nC9                   G                      Am7          F9 \\n  Quanto mais eu te adoro mais eu quero ser Teu, Teu, Teu \\n\\nAm7        F9                C9               G/B \\n  Te conhecia de ouvir falar,  mas agora de contigo andar \\nAm7       G                       F9 \\n  Eu sei,  que não há Deus como o nosso Deus \\n\\n    C9        G          F9      Am7       G          F9 \\nNão há nenhum outro como Deus Não há nenhum outro como Deus \\n    Am7       G          F9      Am7       G          F9 \\nTu és maior, és o autor da fé Não há nenhum outro como Deus \\n\\nC9  G  Am7  F9  C9  G  Am7  F9 \\n\\nC9                G \\nOh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh  \\nAm7               F9                |2x \\nOh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh \\n\\n    C9        G          F9      Am7       G          F9 \\nNão há nenhum outro como Deus Não há nenhum outro como Deus \\n    Am7       G          F9      Am7       G          F9 \\nTu és maior, és o autor da fé Não há nenhum outro como Deus \\n\\nC9                G \\nOh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh  \\nAm7               F9                |2x \\nOh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh \\n\\nVamos lá igreja... \\n\\nC9                G \\nOh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh  \\nAm7               F9                |2x \\nOh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh \\n\\nC9                G \\nOh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh  \\nAm7               F9             C9   |2x \\nOh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh', 'label': 'gospel'}\n",
      "{'text': 'A                      D \\nJesus eu te agradeço por tudo que me fez \\n      Bm                      E  \\nPelo sangue derramado lá na cruz \\n   A             A7           D             Dm     \\nJesus eu te agradeço por me amar e me escolher \\n    A           E                  D          E   A  E \\nMe libertou me restaurou, hoje eu venho te agradecer \\n\\n     A                                  D \\nObrigado Jesus por minha família pelos meus irmãos \\n     Bm             E       E7   A  E                  \\nObrigado Jesus pela água e pelo pão \\n     A                  A7        D Dm   \\nObrigado Jesus mais um dia se passou \\n    A              E  \\nPosso cantar posso adorar \\n     D    E     A  E \\nObrigado meu Jesus \\n\\n   A                      D \\nJesus eu te agradeço pela paz e comunhão \\n       Bm            E           A            A7  \\nPor estar em tua presença pela graça da salvação \\n       D           E          D#m       F#m \\nPelo amor que revelaste e eu sinto pela fé \\n     Bm               D          E   A \\nPor tudo que és hoje venho te agradecer', 'label': 'gospel'}\n",
      "{'text': 'G                  Bm \\nO meu caminho para Deus eu entreguei  \\n C                 Am             D \\nE das ciladas do inimigo me livrei  \\nG                    Bm       \\nPra São Miguel eu conduzi minha oração  \\nC                  Am            D \\nE confiei na poderosa intercessão \\n\\nG                       Em \\nDo mal liberta-me, São Miguel  \\n                         C \\nDo inimigo livrai, São Miguel \\n          Am             D \\nDas tempestades nos socorrei  \\n   \\nG                        Em \\nPerseguições salvai, São Miguel  \\n                         C \\nE dos perigos nos protegeis  \\n        Am                D   \\nNós confiamos em ti São Miguel \\n\\n             F \\nQuem como Deus? \\n             B7 \\nNinguém como Deus!  \\n\\nG                              Em \\nSão Miguel defendei-nos no combate  \\n                  C \\nSeja o nosso protetor \\n     Am             D \\nNos caminhos do Senhor  \\n\\nG \\nQuem como Deus?', 'label': 'gospel'}\n",
      "{'text': 'D                                  A \\nQuando eu vejo alguém na igreja chorando \\n    G                     F#m \\nMe alegro com vontade de chorar \\n D                               F#m \\nÉ o espírito de Deus que esta falando \\n                  D                       A \\nAo seu povo ministrando ao que quer realizar \\nD                               F#m \\nMas o servo que já tem a experiência \\nG                            F#m \\nEle sabe que chorar não é em vão \\nD                               F#m   \\nCada lágrima que desce Deus do céu se compadece \\n        D \\nEle caminha da vitória da oração \\nD                    A                        G         \\nVocê já tem experiência você lembra daquela benção \\n                   D                                 A \\nQue no choro conquistou, o povo do mundo chora com razão \\n                          G                     D \\nMas o servo não chora em vão, o servo tem consolador \\nD                            A                          F#m    \\nOs mais belos livros de poesias foram escritos sem tribulação \\nD                              F#m \\nSinta agora a mãos feridas de Jesus dando guarida \\nD                           A \\nEnxugando o seu rosto meu irmão \\nD                            F#m \\nNada pode resistir a mão de Deus \\nG                            D \\nEle é tudo e sobre todos é Senhor \\nD                                   F#m \\nMas tem uma arma certa que Ele não pode resistir \\n       D \\nÉ um coração quebrantado chorando assim', 'label': 'gospel'}\n",
      "{'text': 'E \\nOlhando pra mim \\nC#m                  A \\nSó posso enxergar as minhas limitações \\n E \\nEm volta de mim \\nC#m \\nTudo quer me dizer \\nA \\nQue eu não irei crescer \\nA                        C#m \\nMas quando me lembro que tenho um dono \\n       A      B    C#m   B \\nE pra Ele não há limite algum \\nA                    C#m \\nNem tempo ou espaço, nem pecado ou vaso \\nA           B           C#m        B \\nEntão eu descanso e me lanço, pois sei \\nA           C#m \\nSou Dele e Ele é meu \\nA            B \\nSou Dele e Ele é meu \\nE \\nMe rendo, me entrego \\n     C#m        B \\nPois sei que em Ti \\n   A   B        E \\nEu sou e posso mais \\nE                       C#m \\nSó quero, preciso de Ti \\n      B       A    B     E \\nMeu rei, meu dono e meu Pai', 'label': 'gospel'}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create generator\n",
    "def my_gen(folder):\n",
    "    path = Path(folder)\n",
    "    for file in path.glob('**/*'):\n",
    "        if file.is_file():\n",
    "            with open(str(file), \"r\") as f:\n",
    "                 yield {\"text\": f.read(), \"label\": str(file.parent).split(\"/\")[-1] }\n",
    "\n",
    "aux = 0\n",
    "for item in my_gen(\"selected_cifras_test\"):\n",
    "    print(item)\n",
    "    aux += 1\n",
    "    if aux > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df338d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 70\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "ds_test = Dataset.from_generator(my_gen, gen_kwargs={\"folder\": \"selected_cifras_test\"})\n",
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d00933ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Intr:B F# C# B F# \\n\\n 1ª Parte: \\n         F# \\n Elas dizem que sou galinha \\n     B \\n Que ando fora da linha \\n        F#                  C# \\n Que a minha praia é a gandaia \\n               F# \\n Que eu tenho cara de safado \\n       B \\n Mulherengo, assanhado \\n    F#        C#      F# \\n Viciado num rabo de saia \\n    C# \\n Depois me procuram \\n      B \\n Me chamam de amor \\n     F# \\n Me pedem carinho \\n\\n E é claro que eu dou \\n    C#                            B          F# \\n Meninas tenham calma que eu não sou de ninguém \\n                  C#          F# B \\n Tem lugar pra todas no meu arém \\n\\n Refrão:(2x) \\n             F#          C#        B \\n Deve ser o mel que a mamãe me passou \\n  C#         F#           C#         B \\n Deve ser o céu que elas pedem e eu dou \\n      C#             D#m          C#           B \\n No amor eu tenho o dom em cada flecha um coração \\n     F#          C#       F# \\n Eu sou o robin hood da paixão \\n\\n Solo:B F# C# B F# D \\n\\n       G \\n Elas dizem que sou galinha \\n     C \\n Que ando fora da linha \\n        G                  D \\n Que a minha praia é a gandaia \\n               G \\n Que eu tenho cara de safado \\n       C \\n Mulherengo, assanhado \\n     G        D       G \\n Viciado num rabo de saia \\n    D \\n Depois me procuram \\n      C \\n Me chamam de amor \\n     G \\n Me pedem carinho \\n\\n E é claro que eu dou \\n    D                             C          G \\n Meninas tenham calma que eu não sou de ninguém \\n                      D       G  D \\n Tem lugar pra todas no meu arém \\n\\n Refrão:(2x) \\n            G           D         C \\n Deve ser o mel que a mamãe me passou \\n  D          G            D          C \\n Deve ser o céu que elas pedem e eu dou \\n      D              Em           D            C \\n No amor eu tenho o dom em cada flecha um coração \\n     G           D        G \\n Eu sou o robin hood da paixão \\n\\n        G           D        G \\n ...eu sou o robin hood da paixão \\n     G           D        G    G \\n Eu sou o robin hood da paixão.',\n",
       " 'label': 'mpb'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf27586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In fact, we need two datasets, for training and for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ad4ca61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 8400\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2100\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "train_ds = Dataset.from_generator(my_gen, gen_kwargs={\"folder\": \"selected_cifras/train\"})\n",
    "test_ds = Dataset.from_generator(my_gen, gen_kwargs={\"folder\": \"selected_cifras/test\"})\n",
    "\n",
    "ds = DatasetDict({\"train\": train_ds, \"test\": test_ds})\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44bbc143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95a0a1a23d34e248775dc57aa941a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(samples):\n",
    "    return tokenizer(samples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_ds = ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a7b8807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 8400\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e67dbe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e23ed2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5e88ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1708576",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0:'gospel',1:'sertanejo',2:'mpb',3:'samba-e-pagode',4:'forro',5:'pop-music',6:'rockn-roll'}\n",
    "label2id = {'gospel':0,'sertanejo':1,'mpb':2,'samba-e-pagode':3,'forro':4,'pop-music':5,'rockn-roll':6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66182639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=7, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4bc4426",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`label` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:748\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 748\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:720\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[0;32m--> 720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 24\u001b[0m\n\u001b[1;32m      1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     push_to_hub\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     16\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 24\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.8/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.8/site-packages/transformers/trainer.py:1838\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1835\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1837\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1838\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1839\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1840\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.8/site-packages/accelerate/data_loader.py:452\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.8/site-packages/transformers/data/data_collator.py:249\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 249\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    257\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3295\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3292\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3293\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m-> 3295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:223\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    219\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:764\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    760\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    761\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    762\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    763\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    765\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    766\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    767\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    768\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    769\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`label` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9292e828",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
